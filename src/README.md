Lab assignment #2
Neural Machine Translation on EN-RU dataset
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/natural-language-processing/22s_made/homeworks/lab02_neural_machine_translation/lab02_Neural_Machine_Translation.ipynb)
  
  
  Вторая лабораторная работа уже доступна в репозитории курса.
**Ее основные цели**: построить систему машинного перевода (с русского языка на английский или же наоборот); провести серию экспериментов, в которых проработать различные подходы и сравнить их между собой; и, конечно, получить практический опыт построения модульного решения на примере задачи машинного перевода.

Вас ждет достаточно широкий простор для экспериментов и выбор из различных подходов. Например, вы можете обратиться к одной из следующих идей (но ими список не ограничивается):
 - Использовать сверточную, рекуррентную или Transformer-like сеть (в том числе и BERT) в качестве энкодера
 - Добавить механизм Attention между энкодером и декодером
 - Использовать предобученные векторные представления слов. Для русского языка, например, можно воспользоваться наработками DeepPavlov
 - Предобучить энкодер и декодер на соответствующих языках используя дополнительные корпуса текстов
 - Формулировка собственных гипотез и идей (конечно, с описанием самой идеи) максимально приветствуется!
 - Основное ограничение: использование того или иного подхода должно быть обосновано, а результаты эксперимента тщательно проанализированы.

Рекомендации по оформлению экспериментов:
Ниже приведено несколько рекомендаций по проведению и оформлению экспериментов:
 - Перенесите класс модели в отдельный файл (и сопутствующий код). Baseline-решение доступно в файле my_network.py.
 - Реализуйте и используйте общий pipeline для обучения моделей
 - Обучение сложных моделей полезно запускать используя специальный .py файл (если у вас есть доступ к CLI)
 - Для мониторинга поведения модели можно воспользоваться Tensorboard или схожими решениями
 
В основном ноутбуке оставьте описания экспериментов, графики, иллюстрирующие полученные результаты, примеры переводов и любую дополнительную информацию
Основные требования к экспериментам:
 - Пожалуйста, проведите не менее четырех различных экспериментов (степень различия может варьироваться). В каждом эксперименте:
 - Опишите основную идею эксперимента, проиллюстрируйте процесс обучения с помощью графиков
 - Оцените полученные результаты перевода используя подходящие критерии качества (Perplexity/BLEU/...)
 - Приведите удачные и неудачные примеры перевода (по 2-3 предложения будет достаточно)
 - Оцените скорость обучения модели (с точки зрения затраченного времени) и количество пар использованных примеров (число эпох * число обработанных батчей за одну эпоху * размер батча)
 - Оцените степень переобучения модели
 - Оцените скорость применения модели в режиме inference в пересчете на один батч размером 32 (если размер батча необходимо уменьшить, например, до 16, то умножьте усредненный результат на 32/16 = 2)

***Пример интересных различных экспериментов***: 
 - использование предобученного на исходном языке энкодера в одном эксперименте и обучение аналогичной архитектуры на задачу машинного перевода с нуля в другом.

***Пример не слишком различных экспериментов***: 
 - использование однослойной LSTM в качестве декодера в одном эксперименте и двухслойной в другом.

Рекомендую хотя бы в одном из экспериментов попробовать следующие подходы:
 - Использовать Transformer decoder в роли декодера (или, например, GPT-like модель)
 - Обучить энкодер с нуля или же ипользовать предобученный на задаче языкового моделирования вариант
 - Использовать контекстных эмбеддингов в энкодере (например, BERT)

Общие рекомендации:
Внимание! При выполнении задания на Colab не забывайте сохранять локально/в Google drive как результаты экспериментов, так и код/веса обученных моделей. Файлы, которые хранятся только в запущенной сессии Colab могут быть утеряны, что неприятно.
