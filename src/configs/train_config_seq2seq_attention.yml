dataparams:
  path_to_data: "./data/raw/data_little.txt"
  file_format: "tsv"
  split_ratio: [0.8, 0.15, 0.05]
  min_word_freq: 3
  batch_size: 128
seq2seqparams:
  model_type: "seq2seq_attention"
  enc_emb_dim: 8
  dec_emb_dim: 8
  enc_hid_dim: 16
  dec_hid_dim: 16
  enc_dropout: 0.35
  dec_dropout: 0.35
  n_layers: 2
  save_path: "./src/models/saved_states/seq2seq_attention.pt"
trainparams:
  clip: 1
  num_epochs: 1
  teacher_forcing_ratio: 0.5
  translated_examples_file: "./reports/seq2seq_attention_generated.txt"