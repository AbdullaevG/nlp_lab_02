{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6156d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, num_layers=self.n_layers, bidirectional=True)\n",
    "        self.fc_hid = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.fc_cell = nn.Linear(enc_hid_dim * 2, dec_hid_dim)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        #src = [src_len, batch_size] \n",
    "        embedded = self.dropout(self.embedding(inp))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]        \n",
    "        outputs, (hidden, cell )= self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src_len, batch_size, hid_dim * num_directions]\n",
    "        #hidden = [n_layers * num directions, batch_size, hid_dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc_hid(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        cell = torch.tanh(self.fc_cell(torch.cat((cell[-2,:,:], cell[-1,:,:]), dim = 1)))\n",
    "        #outputs = [src_len, batch_size, enc_hid_dim * 2] - concat hid vector for every token\n",
    "        #hidden = [batch_size, dec_hid_dim] - changed with fc hidden state from last step \n",
    "        \n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear(2*enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        print(hidden.shape, encoder_outputs.shape)\n",
    "        #torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
    "        \n",
    "        #hidden = [batch_size, dec_hid_dim] - hidden state from previous state of decoder\n",
    "        #encoder_outputs = [src_len, batch_size, enc_hid_dim * 2] - all hidden states from encoder\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #hidden = [batch_size, src_len, dec_hid_dim]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "        print(\"attention\", hidden.shape, encoder_outputs.shape, torch.cat((hidden, encoder_outputs), dim = 2).shape)\n",
    "        # torch.Size([128, 39, 32]) torch.Size([128, 39, 64])\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
    "        #energy = [batch_size, src_len, dec_hid_dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)     \n",
    "        #attention= [batch_size, src_len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch_size, dec_hid_dim]\n",
    "        #encoder_outputs = [src_len, batch_size, enc_hid_dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)             \n",
    "        #a = [batch_size, src_len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch_size, 1, src_len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  \n",
    "        #encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)   \n",
    "        #weighted = [batch_size, 1, enc_hid_dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)     \n",
    "        #weighted = [1, batch_size, enc_hid_dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2) \n",
    "        #rnn_input = [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
    "            \n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq_len, n_layers and n_directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch_size, dec_hid_dim]\n",
    "        #hidden = [1, batch_size, dec_hid_dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, (hidden.squeeze(0), cell.squeeze(0))\n",
    "    \n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        print(encoder_outputs.shape, (hidden.shape, cell.shape))\n",
    "        # torch.Size([39, 128, 64]) (torch.Size([128, 32]), torch.Size([128, 32]))\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, (hidden, cell) = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "def seq2seq_attention(input_dim,\n",
    "                      output_dim, \n",
    "                      enc_emb_dim, \n",
    "                      dec_emb_dim, \n",
    "                      enc_hid_dim, \n",
    "                      dec_hid_dim,\n",
    "                      enc_dropout, \n",
    "                      dec_dropout,\n",
    "                      n_layers,\n",
    "                      save_path,\n",
    "                      model_type,):\n",
    "    attn = Attention(enc_hid_dim, dec_hid_dim)\n",
    "    enc = Encoder(input_dim, enc_emb_dim, enc_hid_dim, dec_hid_dim,  n_layers, enc_dropout)\n",
    "    dec = Decoder(output_dim, dec_emb_dim, enc_hid_dim, dec_hid_dim, dec_dropout, attn)\n",
    "\n",
    "    model = Seq2Seq(enc, dec, device)\n",
    "    model.apply(init_weights).to(device)\n",
    "    \n",
    "    return model, save_path\n",
    "\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db78a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ece6f6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([39, 128, 64]), (torch.Size([128, 32]), torch.Size([128, 32])))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(len(SRC.vocab),16, 32, 32, 2, 0.5)\n",
    "enc_outputs, (enc_hidden, enc_cell) = encoder(x.src)\n",
    "enc_outputs.shape, (enc_hidden.shape, enc_cell.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ab9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "007b5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_do_data = '../../data/raw/data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dc01481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer_W = WordPunctTokenizer()\n",
    "def tokenize(x, tokenizer=tokenizer_W):\n",
    "    return tokenizer.tokenize(x.lower())\n",
    "\n",
    "SRC = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "dataset = torchtext.data.TabularDataset(\n",
    "    path=path_do_data,\n",
    "    format='tsv',\n",
    "    fields=[('trg', TRG), ('src', SRC)]\n",
    ")\n",
    "\n",
    "train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.15, 0.05])\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 3)\n",
    "TRG.build_vocab(train_data, min_freq = 3)\n",
    "\n",
    "def _len_sort_key(x):\n",
    "    return len(x.src)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = \"cpu\",\n",
    "    sort_key=_len_sort_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6c586e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.trg]:[torch.LongTensor of size 39x128]\n",
      "\t[.src]:[torch.LongTensor of size 39x128]\n",
      "torch.Size([39, 128]) torch.Size([39, 128])\n"
     ]
    }
   ],
   "source": [
    "for x in train_iterator:\n",
    "    break\n",
    "print(x)\n",
    "print(x.src.shape, x.trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c50e0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, save_path = seq2seq_attention(len(SRC.vocab),\n",
    "                          len(TRG.vocab),\n",
    "                          16,\n",
    "                          16,\n",
    "                          32,\n",
    "                          32,\n",
    "                          0.5,\n",
    "                          0.5,\n",
    "                          2,\n",
    "                          \"./model.pt\",\n",
    "                          \"seq2seq_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cee71f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 128, 64]) (torch.Size([128, 32]), torch.Size([128, 32]))\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n",
      "torch.Size([128, 32]) torch.Size([39, 128, 64])\n",
      "attention torch.Size([128, 39, 32]) torch.Size([128, 39, 64]) torch.Size([128, 39, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 4.3265e-04,  2.2280e-04,  3.3480e-04,  ...,  1.4825e-04,\n",
       "           1.5613e-04,  1.9725e-04],\n",
       "         [ 8.3401e-04, -5.2487e-05,  8.1793e-04,  ..., -1.6827e-04,\n",
       "           2.3982e-04,  1.1151e-04],\n",
       "         [-1.0579e-04,  1.5802e-04, -2.6626e-04,  ...,  6.7232e-04,\n",
       "           1.2222e-03,  1.2902e-04],\n",
       "         ...,\n",
       "         [-2.1160e-04,  5.8932e-04,  3.2670e-04,  ...,  7.6783e-04,\n",
       "           8.8562e-04, -2.3454e-04],\n",
       "         [ 6.1976e-04,  1.8036e-04,  5.3854e-04,  ...,  3.5865e-04,\n",
       "           8.2230e-04, -2.5239e-04],\n",
       "         [-8.9455e-04, -1.1253e-03,  5.8613e-04,  ..., -4.7579e-04,\n",
       "           5.6388e-04, -5.4944e-04]],\n",
       "\n",
       "        [[ 1.6374e-03,  8.5145e-04,  5.2496e-04,  ...,  3.1947e-04,\n",
       "          -1.7055e-04,  4.5676e-04],\n",
       "         [ 7.7971e-04,  4.7123e-04,  2.3424e-04,  ...,  5.1114e-04,\n",
       "          -9.8851e-05,  2.2258e-04],\n",
       "         [ 7.2862e-04,  3.6913e-04,  4.0900e-04,  ...,  1.6489e-04,\n",
       "           5.7562e-05,  2.8361e-04],\n",
       "         ...,\n",
       "         [ 9.0299e-04, -4.8959e-05, -2.6268e-04,  ...,  8.4702e-04,\n",
       "          -4.3207e-04,  8.5574e-04],\n",
       "         [ 9.4500e-04,  9.5761e-04, -7.8661e-04,  ..., -2.9627e-04,\n",
       "           1.3181e-04, -7.4767e-04],\n",
       "         [-2.2737e-05,  4.0169e-05,  1.9920e-06,  ..., -8.9840e-04,\n",
       "           4.8745e-04, -4.2048e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.0257e-04, -3.4221e-04,  9.9334e-04,  ..., -4.4756e-05,\n",
       "          -7.9940e-04,  2.3868e-04],\n",
       "         [-3.9436e-04, -5.7773e-04,  7.4697e-04,  ..., -5.4334e-04,\n",
       "          -6.3486e-04, -4.4410e-05],\n",
       "         [-5.3976e-04, -8.4378e-04,  1.7646e-03,  ..., -9.1146e-04,\n",
       "           8.9414e-05, -2.5638e-04],\n",
       "         ...,\n",
       "         [-4.0351e-04, -9.8001e-04, -2.0802e-04,  ..., -1.2061e-05,\n",
       "          -1.0982e-04,  3.5767e-04],\n",
       "         [-6.3445e-05, -6.4592e-04, -2.9545e-04,  ..., -5.6250e-05,\n",
       "          -2.8625e-04,  2.9828e-04],\n",
       "         [-1.7948e-04, -7.9411e-04,  5.9170e-04,  ..., -4.2491e-04,\n",
       "          -3.9802e-05,  2.5149e-04]],\n",
       "\n",
       "        [[-2.6631e-04,  1.3969e-04, -1.5024e-04,  ...,  2.7560e-04,\n",
       "          -4.9023e-04,  5.8735e-07],\n",
       "         [ 4.1199e-04, -6.2245e-04,  4.8698e-04,  ..., -4.1862e-04,\n",
       "           2.0841e-04,  1.0523e-04],\n",
       "         [-8.1864e-05,  5.1402e-04,  7.0748e-04,  ...,  2.0752e-04,\n",
       "          -4.0998e-04, -1.7372e-04],\n",
       "         ...,\n",
       "         [ 3.0064e-04,  5.3707e-04,  3.9125e-04,  ..., -1.0108e-04,\n",
       "           7.1858e-04, -2.3647e-04],\n",
       "         [ 1.3310e-03, -7.4222e-04, -2.6672e-04,  ...,  4.0346e-05,\n",
       "          -3.8697e-04,  1.0105e-03],\n",
       "         [-2.7909e-04,  4.2978e-06, -1.0629e-03,  ...,  4.8928e-04,\n",
       "          -8.2288e-04,  1.0432e-04]],\n",
       "\n",
       "        [[ 4.4292e-04,  6.5937e-04,  1.4977e-04,  ...,  3.5872e-04,\n",
       "          -5.5821e-04,  2.0421e-04],\n",
       "         [ 4.4095e-04, -2.7177e-04, -1.2039e-03,  ...,  1.0009e-03,\n",
       "          -8.0406e-04,  8.7553e-04],\n",
       "         [-3.6077e-04, -3.9981e-04,  1.4446e-03,  ..., -5.7323e-05,\n",
       "          -1.1174e-03,  8.0640e-04],\n",
       "         ...,\n",
       "         [ 6.8210e-04, -1.1500e-04, -9.8601e-05,  ...,  3.0802e-04,\n",
       "          -5.1394e-04,  3.5085e-04],\n",
       "         [-4.0096e-04, -8.8281e-04,  6.1004e-04,  ..., -1.2651e-04,\n",
       "           2.7838e-04,  6.4740e-04],\n",
       "         [-1.8230e-04, -4.0333e-06, -5.4514e-05,  ...,  9.7784e-05,\n",
       "           3.8239e-04, -3.3629e-04]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x.src, x.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacf4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
