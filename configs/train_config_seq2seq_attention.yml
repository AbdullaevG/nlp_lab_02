dataparams:
  path_to_data: "./data/raw/data.txt"
  file_format: "tsv"
  split_ratio: [0.8, 0.15, 0.05]
  min_word_freq: 3
  batch_size: 128
seq2seqparams:
  model_type: "seq2seq_attention"
  enc_emb_dim: 256
  dec_emb_dim: 256
  enc_hid_dim: 512
  dec_hid_dim: 512
  enc_dropout: 0.35
  dec_dropout: 0.35
  n_layers: 2
  save_path: "./src/models/saved_states/seq2seq_attention.pt"
trainparams:
  clip: 1
  num_epochs: 20
  teacher_forcing_ratio: 0.5
  translated_examples_file: "./reports/seq2seq_attention_generated.txt"